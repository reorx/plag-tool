"""Integration test for chunker and embedding service."""

import os
import pytest
import numpy as np
from dotenv import load_dotenv

from plag_tool.core.config import Config
from plag_tool.core.chunker import TextChunker
from plag_tool.core.embeddings import EmbeddingService
from plag_tool.core.log import set_logger

# Load environment variables at module level
load_dotenv()


@pytest.mark.skipif(
    not os.getenv("OPENAI_API_KEY") and not os.getenv("OPENAI_DEFAULT_EMBEDDING_MODEL"),
    reason="OPENAI_API_KEY not configured - skipping integration test"
)
def test_chunker_with_embedding_integration():
    """
    Integration test that combines chunker and embedding service.
    Uses a Chinese text of about 200 characters, chunks it, then embeds the chunks.
    """
    # Configure logging to DEBUG level to see chunk details
    set_logger(
        'plag_tool',
        level='DEBUG',
        fmt='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        remove_handlers=True  # Clear any existing handlers first
    )

    # Setup config and services
    config = Config()

    # Skip if API key is not available
    if not config.validate_api_key():
        pytest.skip("API key not configured")

    # Chinese text about AI and technology (approximately 200 characters)
    chinese_text = """äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œæ·±åº¦å­¦ä¹ ç®—æ³•åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†é‡å¤§çªç ´ã€‚æœºå™¨å­¦ä¹ æ¨¡åž‹çš„æ€§èƒ½ä¸æ–­æå‡ï¼Œä¸ºå„è¡Œå„ä¸šå¸¦æ¥äº†æ–°çš„æœºé‡å’ŒæŒ‘æˆ˜ã€‚éšç€è®¡ç®—èƒ½åŠ›çš„å¢žå¼ºå’Œæ•°æ®é‡çš„å¢žé•¿ï¼Œäººå·¥æ™ºèƒ½åœ¨åŒ»ç–—è¯Šæ–­ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½åˆ¶é€ ç­‰æ–¹é¢å±•çŽ°å‡ºå·¨å¤§æ½œåŠ›ã€‚æœªæ¥äººå·¥æ™ºèƒ½å°†ç»§ç»­æŽ¨åŠ¨ç§‘æŠ€è¿›æ­¥å’Œç¤¾ä¼šå‘å±•ã€‚"""

    print(f"\nðŸ“ Original text ({len(chinese_text)} characters):")
    print(f"'{chinese_text}'")

    # Initialize chunker with reasonable parameters
    chunker = TextChunker(chunk_size=50, overlap=10)
    print(f"\nðŸ”„ Chunking with chunk_size=50, overlap=10")

    # Chunk the text
    chunks = chunker.chunk_text(chinese_text, "ai_article")

    print(f"\nðŸ“Š Created {len(chunks)} chunks:")
    for i, chunk in enumerate(chunks):
        print(f"  Chunk {i+1}: pos {chunk.start_pos}-{chunk.end_pos}, "
              f"text: '{chunk.text}'")

    # Initialize embedding service
    embedding_service = EmbeddingService(config)

    # Embed the chunks with small batch settings to see batching in action
    print(f"\nðŸš€ Embedding chunks with batch limits (tokens=100, items=2)...")
    embeddings = embedding_service.embed_chunks(
        chunks,
        use_cache=False,  # Disable cache to see API calls
        batch_max_tokens=100,  # Small token limit to force multiple batches
        batch_max_items=2,     # Small item limit to force multiple batches
        max_retries=2
    )

    # Verify results
    assert len(embeddings) == len(chunks), f"Expected {len(chunks)} embeddings, got {len(embeddings)}"

    print(f"\nâœ… Verification results:")
    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
        assert isinstance(embedding, np.ndarray), f"Embedding {i} should be numpy array"
        assert embedding.dtype in [np.float32, np.float64], f"Embedding {i} should be float type"
        assert len(embedding) > 0, f"Embedding {i} should have dimensions"
        assert np.all(np.isfinite(embedding)), f"Embedding {i} should have finite values"
        assert not np.allclose(embedding, 0), f"Embedding {i} should not be all zeros"

        print(f"  Chunk {i+1}: âœ“ embedding shape {embedding.shape}, dtype {embedding.dtype}")

    print(f"\nðŸŽ‰ Integration test completed successfully!")
    print(f"   - Processed {len(chinese_text)} character Chinese text")
    print(f"   - Created {len(chunks)} chunks")
    print(f"   - Generated {len(embeddings)} embeddings")
    print(f"   - Embedding dimension: {len(embeddings[0])}")
    print(f"   - Model used: {config.openai_model}")